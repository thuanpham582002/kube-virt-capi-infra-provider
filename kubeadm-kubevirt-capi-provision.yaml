# CAPI Spec ƒë·ªÉ adopt VM c√≥ s·∫µn cho c·∫£ Control Plane v√† Worker
# Control Plane: s·ª≠ d·ª•ng KubeadmControlPlane provider
# Worker: s·ª≠ d·ª•ng MachineDeployment
# VM ƒëang ·ªü managed cluster (c√πng cluster v·ªõi CAPI) - KH√îNG C·∫¶N infraClusterSecretRef

---
# Cluster resource - ƒë·ªãnh nghƒ©a to√†n b·ªô cluster
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: kubeadm-existing-vm-test
  namespace: vm-lab
  labels:
    cluster-type: existing-vm-adoption
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 10.243.0.0/16
    services:
      cidrBlocks:
        - 10.95.0.0/16
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
    kind: KubevirtCluster
    name: kubeadm-existing-vm-test
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: kubeadm-existing-vm-test-control-plane

---
# KubevirtCluster - infrastructure cluster definition
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
kind: KubevirtCluster
metadata:
  name: kubeadm-existing-vm-test
  namespace: vm-lab
spec:
  controlPlaneServiceTemplate:
    spec:
      type: ClusterIP
  # KH√îNG C·∫¶N infraClusterSecretRef v√¨ VM ·ªü c√πng cluster v·ªõi CAPI

---
# KubevirtMachineTemplate cho Control Plane v·ªõi adoption
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
kind: KubevirtMachineTemplate
metadata:
  name: kubeadm-existing-vm-test-control-plane
  namespace: vm-lab
  annotations:
    # üîë ADOPTION ANNOTATION cho Control Plane VM
    capk.cluster.x-k8s.io/existing-vm-name: "lab-vm-04"
spec:
  template:
    spec:
      # Template n√†y ch·ªâ ƒë·ªÉ validation, kh√¥ng t·∫°o VM m·ªõi
      virtualMachineTemplate:
        metadata:
          namespace: vm-lab
        spec:
          runStrategy: Always
          template:
            spec:
              domain:
                cpu:
                  cores: 2
                memory:
                  guest: "4Gi"
                devices:
                  networkInterfaceMultiqueue: true
                  disks:
                    - disk:
                        bus: virtio
                      name: rootdisk
                    - disk:
                        bus: virtio
                      name: cloudinit
                  interfaces:
                    - masquerade: {}
                      name: default
              networks:
                - name: default
                  pod: {}
              volumes:
                - dataVolume:
                    name: lab-vm-04-disk
                  name: rootdisk
                - cloudInitNoCloud:
                    userData: "#cloud-config\n# S·∫Ω ƒë∆∞·ª£c inject CAPI bootstrap data"
                  name: cloudinit
      # KH√îNG C·∫¶N infraClusterSecretRef v√¨ VM ·ªü c√πng cluster

---
# KubeadmControlPlane - qu·∫£n l√Ω control plane lifecycle
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: kubeadm-existing-vm-test-control-plane
  namespace: vm-lab
spec:
  replicas: 0
  machineTemplate:
    infrastructureRef:
      kind: KubevirtMachineTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
      name: kubeadm-existing-vm-test-control-plane
      namespace: vm-lab
  kubeadmConfigSpec:
    # Cluster configuration cho control plane
    clusterConfiguration:
      networking:
        dnsDomain: "kubeadm-existing-vm-test.default.local"
        podSubnet: 10.243.0.0/16
        serviceSubnet: 10.95.0.0/16
      apiServer:
        extraArgs:
          cloud-provider: external
        certSANs:
          - localhost
          - 127.0.0.1
      controllerManager:
        extraArgs:
          cloud-provider: external
    # Init configuration cho first control plane node
    initConfiguration:
      nodeRegistration:
        criSocket: "unix:///var/run/containerd/containerd.sock"
        kubeletExtraArgs:
          cloud-provider: external
          provider-id: "kubevirt://lab-vm-04"
    # Join configuration cho additional control plane nodes
    joinConfiguration:
      nodeRegistration:
        criSocket: "unix:///var/run/containerd/containerd.sock"
        kubeletExtraArgs:
          cloud-provider: external
          provider-id: "kubevirt://lab-vm-04"
  version: v1.28.0

---
# KubevirtMachineTemplate cho Worker nodes v·ªõi adoption
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
kind: KubevirtMachineTemplate
metadata:
  name: kubeadm-existing-vm-test-worker
  namespace: vm-lab
  annotations:
    # üîë ADOPTION ANNOTATION cho Worker VM
    capk.cluster.x-k8s.io/existing-vm-name: "lab-vm-03"
spec:
  template:
    spec:
      virtualMachineTemplate:
        metadata:
          namespace: vm-lab
        spec:
          runStrategy: Always
          template:
            spec:
              domain:
                cpu:
                  cores: 2
                memory:
                  guest: "4Gi"
                devices:
                  networkInterfaceMultiqueue: true
                  disks:
                    - disk:
                        bus: virtio
                      name: rootdisk
                    - disk:
                        bus: virtio
                      name: cloudinit
                  interfaces:
                    - masquerade: {}
                      name: default
              networks:
                - name: default
                  pod: {}
              volumes:
                - dataVolume:
                    name: lab-vm-03-disk
                  name: rootdisk
                - cloudInitNoCloud:
                    userData: "#cloud-config\n# S·∫Ω ƒë∆∞·ª£c inject CAPI bootstrap data"
                  name: cloudinit
      # KH√îNG C·∫¶N infraClusterSecretRef v√¨ VM ·ªü c√πng cluster

---
# KubeadmConfigTemplate cho Worker bootstrap
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: kubeadm-existing-vm-test-worker
  namespace: vm-lab
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          criSocket: "unix:///var/run/containerd/containerd.sock"
          kubeletExtraArgs:
            cloud-provider: external
            provider-id: "kubevirt://lab-vm-03"

---
# MachineDeployment cho Worker nodes
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: kubeadm-existing-vm-test-worker
  namespace: vm-lab
spec:
  clusterName: kubeadm-existing-vm-test
  replicas: 0
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: kubeadm-existing-vm-test
      cluster.x-k8s.io/deployment-name: worker-existing-vm
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: kubeadm-existing-vm-test
        cluster.x-k8s.io/deployment-name: worker-existing-vm
    spec:
      clusterName: kubeadm-existing-vm-test
      version: v1.28.0
      bootstrap:
        configRef:
          name: kubeadm-existing-vm-test-worker
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
      infrastructureRef:
        name: kubeadm-existing-vm-test-worker
        apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
        kind: KubevirtMachineTemplate

---
# üìù VM MAPPING DOCUMENTATION
# B·∫°n c·∫ßn ƒë·∫£m b·∫£o c√°c VM sau ƒë√£ c√≥ s·∫µn v√† ƒëang ch·∫°y:
#
# Control Plane VM:
#   - T√™n: lab-vm-04
#   - Namespace: default (ho·∫∑c namespace hi·ªán t·∫°i)
#   - Status: Running
#   - Requirements: 2 CPU, 4Gi RAM, guest agent
#
# Worker VM:
#   - T√™n: lab-vm-03
#   - Namespace: default (ho·∫∑c namespace hi·ªán t·∫°i)
#   - Status: Running
#   - Requirements: 2 CPU, 4Gi RAM, guest agent
#
# ƒê·ªÉ ki·ªÉm tra VM c√≥ s·∫µn:
# kubectl get vm lab-vm-04 lab-vm-03 -n <namespace>
#
# ƒê·ªÉ ki·ªÉm tra VM status:
# kubectl get vm <vm-name> -n <namespace> -o yaml
#
# QUAN TR·ªåNG: Sau khi apply, CAPI s·∫Ω t·ª± generate kubeconfig cho workload cluster:
# clusterctl get kubeconfig kubeadm-existing-vm-test --namespace default > workload-cluster.kubeconfig
