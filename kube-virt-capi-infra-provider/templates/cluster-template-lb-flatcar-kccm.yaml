# export CLUSTER_NAME=my-cluster # replace your cluster name here
# export NAMESPACE=default # replace your namespace here
# export KUBERNETES_VERSION=v1.30.5
# export NODE_VM_IMAGE_TEMPLATE="capi-flatcar-3975-2-2-v1.30.5.img" # Flatcar image should be pre-build. Check the project https://github.com/kubernetes-sigs/image-builder for more details
# export CONTROL_PLANE_MACHINE_COUNT=3
# export WORKER_MACHINE_COUNT=3
# export INSTANCE_TYPE=u1.large
# export INSTANCE_PREFERENCE=ubuntu
# export SSH_AUTHORIZED_KEY="ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAI..." # specify an SSH key which should be authorized on all nodes

# envsubst < templates/cluster-template-lb-flatcar.yaml | kubectl apply -f -

---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: "${CLUSTER_NAME}"
  namespace: "${NAMESPACE}"
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 10.243.0.0/16
    services:
      cidrBlocks:
        - 10.95.0.0/16
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
    kind: KubevirtCluster
    name: '${CLUSTER_NAME}'
    namespace: "${NAMESPACE}"
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: '${CLUSTER_NAME}-control-plane'
    namespace: "${NAMESPACE}"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
kind: KubevirtCluster
metadata:
  name: "${CLUSTER_NAME}"
  namespace: "${NAMESPACE}"
spec:
  controlPlaneServiceTemplate:
    spec:
      type: LoadBalancer
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
kind: KubevirtMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-control-plane-${KUBERNETES_VERSION}"
  namespace: "${NAMESPACE}"
spec:
  template:
    spec:
      virtualMachineBootstrapCheck:
        checkStrategy: ssh
      virtualMachineTemplate:
        metadata:
          namespace: "${NAMESPACE}"
        spec:
          instancetype:
            kind: VirtualMachineClusterInstancetype
            name: "${INSTANCE_TYPE}"
          preference:
            kind: VirtualMachineClusterPreference
            name: "${INSTANCE_PREFERENCE}"
          runStrategy: Always
          template:
            spec:
              domain:
                devices:
                  networkInterfaceMultiqueue: true
                  disks:
                    - disk:
                        bus: virtio
                      name: containervolume
              evictionStrategy: External
              volumes:
                - containerDisk:
                    image: "${NODE_VM_IMAGE_TEMPLATE}"
                  name: containervolume
---
kind: KubeadmControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: "${CLUSTER_NAME}-control-plane"
  namespace: "${NAMESPACE}"
spec:
  replicas: ${CONTROL_PLANE_MACHINE_COUNT}
  machineTemplate:
    infrastructureRef:
      kind: KubevirtMachineTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
      name: "${CLUSTER_NAME}-control-plane-${KUBERNETES_VERSION}"
      namespace: "${NAMESPACE}"
  kubeadmConfigSpec:
    format: ignition
    files: []
    ignition:
      containerLinuxConfig:
        additionalConfig: |
          systemd:
            units:
            - name: kubeadm.service
              enabled: true
              dropins:
              - name: 10-flatcar.conf
                contents: |
                  [Unit]
                  Requires=containerd.service
                  After=containerd.service
    # The CAPK automatic SSH key authorization doesn't work for Ignition as it assumes cloud-init
    # is used. We need to explicitly add a key to be able to SSH into cluster nodes.
    users:
    - name: core
      sshAuthorizedKeys:
      - "${SSH_AUTHORIZED_KEY}"
    clusterConfiguration:
      networking:
        dnsDomain: "${CLUSTER_NAME}.${NAMESPACE}.local"
        podSubnet: 10.243.0.0/16
        serviceSubnet: 10.95.0.0/16
    initConfiguration:
      nodeRegistration:
        criSocket: /var/run/containerd/containerd.sock
    joinConfiguration:
      nodeRegistration:
        criSocket: /var/run/containerd/containerd.sock
  version: "${KUBERNETES_VERSION}"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
kind: KubevirtMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0-${KUBERNETES_VERSION}"
  namespace: "${NAMESPACE}"
spec:
  template:
    spec:
      virtualMachineBootstrapCheck:
        checkStrategy: ssh
      virtualMachineTemplate:
        metadata:
          namespace: "${NAMESPACE}"
          labels:
            app: ${CLUSTER_NAME}-md-0-${KUBERNETES_VERSION}
        spec:
          instancetype:
            kind: VirtualMachineClusterInstancetype
            name: "${INSTANCE_TYPE}"
          preference:
            kind: VirtualMachineClusterPreference
            name: "${INSTANCE_PREFERENCE}"
          runStrategy: Always
          template:
            metadata:
              labels:
                app: ${CLUSTER_NAME}-md-0-${KUBERNETES_VERSION}
            spec:
              affinity:
                podAntiAffinity: ## set the anti-affinity rule to spread the pods across nodes
                  preferredDuringSchedulingIgnoredDuringExecution: ## pods will be scheduled on the same node if number if nodes are not matching the number of replicas
                  - weight: 100
                    podAffinityTerm:
                      labelSelector:
                        matchExpressions:
                        - key: app
                          operator: In
                          values:
                          - ${CLUSTER_NAME}-md-0-${KUBERNETES_VERSION}
                      topologyKey: kubernetes.io/hostname   
              domain:
                devices:
                  networkInterfaceMultiqueue: true
                  disks:
                    - disk:
                        bus: virtio
                      name: containervolume
              evictionStrategy: External
              volumes:
                - containerDisk:
                    image: "${NODE_VM_IMAGE_TEMPLATE}"
                  name: containervolume
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: "${CLUSTER_NAME}-md-0"
  namespace: "${NAMESPACE}"
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs: {}
      files: []
      format: ignition
      ignition:
        containerLinuxConfig:
          additionalConfig: |
            systemd:
              units:
              - name: kubeadm.service
                enabled: true
                dropins:
                - name: 10-flatcar.conf
                  contents: |
                    [Unit]
                    Requires=containerd.service
                    After=containerd.service
      # The CAPK automatic SSH key authorization doesn't work for Ignition as it assumes cloud-init
      # is used. We need to explicitly add a key to be able to SSH into cluster nodes.
      users:
      - name: core
        sshAuthorizedKeys:
        - "${SSH_AUTHORIZED_KEY}"
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: "${CLUSTER_NAME}-md-0"
  namespace: "${NAMESPACE}"
spec:
  clusterName: "${CLUSTER_NAME}"
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels:
  template:
    metadata:
      labels:
        node-role.kubernetes.io/worker: ''
    spec:
      clusterName: "${CLUSTER_NAME}"
      version: "${KUBERNETES_VERSION}"
      bootstrap:
        configRef:
          name: "${CLUSTER_NAME}-md-0"
          namespace: "${NAMESPACE}"
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
      infrastructureRef:
        name: "${CLUSTER_NAME}-md-0-${KUBERNETES_VERSION}"
        namespace: "${NAMESPACE}"
        apiVersion: infrastructure.cluster.x-k8s.io/v1alpha1
        kind: KubevirtMachineTemplate
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: ${CLUSTER_NAME}-control-plane
  namespace: "${NAMESPACE}"
spec:
  clusterName: ${CLUSTER_NAME}
  maxUnhealthy: 40%
  nodeStartupTimeout: 10m
  selector:
    matchLabels:
      cluster.x-k8s.io/control-plane-name: ${CLUSTER_NAME}-control-plane
  unhealthyConditions:
  - type: Ready
    status: Unknown
    timeout: 300s
  - type: Ready
    status: "False"
    timeout: 300s
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: ${CLUSTER_NAME}-md-0
  namespace: "${NAMESPACE}"
spec:
  clusterName: ${CLUSTER_NAME}
  maxUnhealthy: 100%
  nodeStartupTimeout: 10m
  selector:
    matchLabels:
      cluster.x-k8s.io/deployment-name: ${CLUSTER_NAME}-md-0
  unhealthyConditions:
  - type: Ready
    status: Unknown
    timeout: 300s
  - type: Ready
    status: "False"
    timeout: 300s
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    capk.cluster.x-k8s.io/template-kind: extra-resource
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
  name: cloud-controller-manager
  namespace: ${NAMESPACE}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    capk.cluster.x-k8s.io/template-kind: extra-resource
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
  name: kccm
  namespace: ${NAMESPACE}
rules:
- apiGroups:
  - kubevirt.io
  resources:
  - virtualmachines
  verbs:
  - get
  - watch
  - list
- apiGroups:
  - kubevirt.io
  resources:
  - virtualmachineinstances
  verbs:
  - get
  - watch
  - list
  - update
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    capk.cluster.x-k8s.io/template-kind: extra-resource
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
  name: kccm-sa
  namespace: ${NAMESPACE}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kccm
subjects:
- kind: ServiceAccount
  name: cloud-controller-manager
  namespace: ${NAMESPACE}
---
apiVersion: v1
data:
  cloud-config: |
    loadBalancer:
      creationPollInterval: 5
      creationPollTimeout: 60
    namespace: ${NAMESPACE}
    instancesV2:
      enabled: true
      zoneAndRegionEnabled: false
kind: ConfigMap
metadata:
  labels:
    capk.cluster.x-k8s.io/template-kind: extra-resource
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
  name: cloud-config
  namespace: ${NAMESPACE}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    capk.cluster.x-k8s.io/template-kind: extra-resource
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
    k8s-app: kubevirt-cloud-controller-manager
  name: kubevirt-cloud-controller-manager
  namespace: ${NAMESPACE}
spec:
  replicas: 1
  selector:
    matchLabels:
      capk.cluster.x-k8s.io/template-kind: extra-resource
      cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
      k8s-app: kubevirt-cloud-controller-manager
  template:
    metadata:
      labels:
        capk.cluster.x-k8s.io/template-kind: extra-resource
        cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
        k8s-app: kubevirt-cloud-controller-manager
    spec:
      containers:
      - args:
        - --cloud-provider=kubevirt
        - --cloud-config=/etc/cloud/cloud-config
        - --kubeconfig=/etc/kubernetes/kubeconfig/value
        - --authentication-skip-lookup=true
        - --cluster-name="${CLUSTER_NAME}"
        command:
        - /bin/kubevirt-cloud-controller-manager
        image: quay.io/kubevirt/kubevirt-cloud-controller-manager:v0.5.1
        imagePullPolicy: Always
        name: kubevirt-cloud-controller-manager
        resources:
          requests:
            cpu: 100m
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /etc/kubernetes/kubeconfig
          name: kubeconfig
          readOnly: true
        - mountPath: /etc/cloud
          name: cloud-config
          readOnly: true
      nodeSelector:
        node-role.kubernetes.io/master: ""
      serviceAccountName: cloud-controller-manager
      tolerations:
      - effect: NoSchedule
        key: node.cloudprovider.kubernetes.io/uninitialized
        value: "true"
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      volumes:
      - configMap:
          name: cloud-config
        name: cloud-config
      - name: kubeconfig
        secret:
          secretName: ${CLUSTER_NAME}-kubeconfig
